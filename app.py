from fastapi import FastAPI, Query, HTTPException, BackgroundTasks, Request

from fastapi.responses import FileResponse, RedirectResponse

from fastapi.middleware.cors import CORSMiddleware

import yt_dlp

import requests

import subprocess

import os

import uuid

import logging

import timeÂ 

from urllib.parse import urlencode



# --- âœ¨ NEW: SECURELY HANDLE COOKIES FROM A GIST URL ---

# Define the path for our temporary cookie file

COOKIE_FILE_PATH = "/tmp/cookies.txt"



# Read the secret Gist URL from the environment variable set in Railway

cookie_gist_url = os.getenv("COOKIE_GIST_URL")



# If the URL exists, download its content and write it to the temporary file

if cookie_gist_url:

Â  Â  try:

Â  Â  Â  Â  response = requests.get(cookie_gist_url)

Â  Â  Â  Â  response.raise_for_status() # Raise an exception for bad status codes

Â  Â  Â  Â  with open(COOKIE_FILE_PATH, "w") as f:

Â  Â  Â  Â  Â  Â  f.write(response.text)

Â  Â  Â  Â  logging.info("Successfully loaded cookies from Gist.")

Â  Â  except requests.exceptions.RequestException as e:

Â  Â  Â  Â  logging.error(f"Failed to download cookies from Gist: {e}")

Â  Â  Â  Â  # Create an empty file so the app doesn't crash

Â  Â  Â  Â  open(COOKIE_FILE_PATH, 'a').close()

else:

Â  Â  # If the env var isn't set, create an empty file

Â  Â  logging.warning("COOKIE_GIST_URL not set. Proceeding without cookies.")

Â  Â  open(COOKIE_FILE_PATH, 'a').close()

# --- END OF NEW SECTION ---





logging.basicConfig(level=logging.INFO)

app = FastAPI()



app.add_middleware(

Â  Â  CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"]

)



def cleanup_files(paths: list):

Â  Â  for path in paths:

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  if os.path.exists(path):

Â  Â  Â  Â  Â  Â  Â  Â  os.remove(path)

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  logging.error(f"Error cleaning up file {path}: {e}")



@app.get("/")

def home():

Â  Â  return {"message": "Video Downloader API is running ðŸš€"}



@app.get("/info")

async def get_video_info(request: Request, url: str):

Â  Â  ydl_opts = {'quiet': True}

Â  Â Â 

Â  Â  # --- âœ¨ NEW: Automatic Retry Logic ---

Â  Â  info = None

Â  Â  last_exception = None

Â  Â  for attempt in range(3): # Try up to 3 times

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  with yt_dlp.YoutubeDL(ydl_opts) as ydl:

Â  Â  Â  Â  Â  Â  Â  Â  info = ydl.extract_info(url, download=False)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # If we successfully get the info, stop retrying

Â  Â  Â  Â  Â  Â  if info:

Â  Â  Â  Â  Â  Â  Â  Â  logging.info(f"Successfully fetched info for {url} on attempt {attempt + 1}")

Â  Â  Â  Â  Â  Â  Â  Â  breakÂ 

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  last_exception = e

Â  Â  Â  Â  Â  Â  logging.warning(f"Attempt {attempt + 1} failed for {url}. Retrying in 1 second...")

Â  Â  Â  Â  Â  Â  time.sleep(1) # Wait 1 second before the next attempt



Â  Â  # If all retries failed, raise an error

Â  Â  if not info:

Â  Â  Â  Â  logging.error(f"All retry attempts failed for {url}. Last error: {last_exception}")

Â  Â  Â  Â  raise HTTPException(status_code=500, detail=f"Could not retrieve video information after multiple attempts. The link may be private or invalid.")

Â  Â  # --- End of Retry Logic ---



Â  Â  try:

Â  Â  Â  Â  all_formats = []

Â  Â  Â  Â  video_only = [f for f in info.get('formats', []) if f.get('vcodec') != 'none' and f.get('acodec') == 'none' and f.get('url')]

Â  Â  Â  Â  audio_only = [f for f in info.get('formats', []) if f.get('acodec') != 'none' and f.get('vcodec') == 'none' and f.get('url')]



Â  Â  Â  Â  if video_only and audio_only:

Â  Â  Â  Â  Â  Â  query_params = urlencode({'url': url})

Â  Â  Â  Â  Â  Â  merge_url = str(request.base_url) + "merge_streams?" + query_params

Â  Â  Â  Â  Â  Â  all_formats.append({

Â  Â  Â  Â  Â  Â  Â  Â  'quality': 'Best Quality (Merged)', 'ext': 'mp4', 'url': merge_url,

Â  Â  Â  Â  Â  Â  Â  Â  'vcodec': 'merged', 'acodec': 'merged', 'height': max(v.get('height', 0) for v in video_only)

Â  Â  Â  Â  Â  Â  })



Â  Â  Â  Â  for f in info.get("formats", []):

Â  Â  Â  Â  Â  Â  if f.get("url"):

Â  Â  Â  Â  Â  Â  Â  Â  all_formats.append({

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "url": f.get("url"), "ext": f.get("ext"), "height": f.get("height"),

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "acodec": f.get("acodec"), "vcodec": f.get("vcodec"),

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "quality": f.get("format_note") or (f.get("height") and f"{f.get('height')}p") or "Audio",

Â  Â  Â  Â  Â  Â  Â  Â  })

Â  Â  Â  Â Â 

Â  Â  Â  Â  sorted_formats = sorted(all_formats, key=lambda x: x.get('height') or 0, reverse=True)



Â  Â  Â  Â  return {

Â  Â  Â  Â  Â  Â  "title": info.get("title"), "thumbnail": info.get("thumbnail"), "formats": sorted_formats

Â  Â  Â  Â  }

Â  Â  except Exception as e:

Â  Â  Â  Â  logging.error(f"Error processing formats for URL {url}: {e}")

Â  Â  Â  Â  raise HTTPException(status_code=500, detail="Successfully fetched video, but failed to process formats.")





@app.get("/merge_streams")

async def merge_streams(url: str, background_tasks: BackgroundTasks):

Â  Â  try:

Â  Â  Â  Â  with yt_dlp.YoutubeDL({'quiet': True}) as ydl:

Â  Â  Â  Â  Â  Â  info = ydl.extract_info(url, download=False)

Â  Â  Â  Â Â 

Â  Â  Â  Â  best_video = max((f for f in info['formats'] if f.get('vcodec') != 'none' and f.get('acodec') == 'none'), key=lambda x: x.get('height', 0))

Â  Â  Â  Â  best_audio = max((f for f in info['formats'] if f.get('acodec') != 'none' and f.get('vcodec') == 'none'), key=lambda x: x.get('abr', 0))



Â  Â  Â  Â  request_id = str(uuid.uuid4())

Â  Â  Â  Â  video_path, audio_path, output_path = f"/tmp/{request_id}_v.mp4", f"/tmp/{request_id}_a.m4a", f"/tmp/{request_id}_o.mp4"

Â  Â  Â  Â Â 

Â  Â  Â  Â  files_to_cleanup = [video_path, audio_path, output_path]

Â  Â  Â  Â  background_tasks.add_task(cleanup_files, files_to_cleanup)



Â  Â  Â  Â  with requests.get(best_video['url'], stream=True) as r:

Â  Â  Â  Â  Â  Â  r.raise_for_status()

Â  Â  Â  Â  Â  Â  with open(video_path, 'wb') as f:

Â  Â  Â  Â  Â  Â  Â  Â  for chunk in r.iter_content(chunk_size=8192): f.write(chunk)

Â  Â  Â  Â  with requests.get(best_audio['url'], stream=True) as r:

Â  Â  Â  Â  Â  Â  r.raise_for_status()

Â  Â  Â  Â  Â  Â  with open(audio_path, 'wb') as f:

Â  Â  Â  Â  Â  Â  Â  Â  for chunk in r.iter_content(chunk_size=8192): f.write(chunk)

Â  Â  Â  Â Â 

Â  Â  Â  Â  ffmpeg_cmd = ['ffmpeg', '-i', video_path, '-i', audio_path, '-c', 'copy', output_path]

Â  Â  Â  Â  process = subprocess.run(ffmpeg_cmd, capture_output=True, text=True)

Â  Â  Â  Â  if process.returncode != 0:

Â  Â  Â  Â  Â  Â  raise HTTPException(status_code=500, detail="Failed to merge files.")



Â  Â  Â  Â  return FileResponse(path=output_path, media_type='video/mp4', filename=f"{info.get('title', 'video')}.mp4")

Â  Â  except Exception as e:

Â  Â  Â  Â  logging.error(f"Error in /merge_streams for URL {url}: {e}")

Â  Â  Â  Â  raise HTTPException(status_code=500, detail=str(e))
